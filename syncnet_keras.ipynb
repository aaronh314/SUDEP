{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from utils import *\n",
    "import math as m\n",
    "from tensorflow.keras.initializers import RandomNormal, RandomUniform, Constant, Zeros\n",
    "from collections import defaultdict\n",
    "from sklearn.preprocessing import *\n",
    "from IPython.display import clear_output\n",
    "from tensorflow.keras.models import Sequential\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import *\n",
    "from tensorflow.keras import regularizers\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from scipy.io import loadmat\n",
    "import scipy\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def yes(a, b):\n",
    "    for i in a:\n",
    "        if i not in b:\n",
    "            return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(path_dir, paths, freq):\n",
    "    X, Y = [], []\n",
    "    included = []\n",
    "    \n",
    "    df = read_file(path)\n",
    "    control = \"C\" in path[path.rfind('\\\\')+2:]\n",
    "    name = path[path.rfind('\\\\')+1:]\n",
    "#     print(name)\n",
    "#     chans = ['f7', 'c3', 'p3', 'f3', 'fp2', 'p4', 'f8', 'c4', 'f4', 'fp1', 'o2', 'fz', 'o1', 'cz', 'a1']\n",
    "#     if not yes(chans, df.columns):\n",
    "#         return [], [], []\n",
    "#     data = df[chans]\n",
    "    if name in d:\n",
    "        print(name, d[name])\n",
    "        data = df[d[name]]\n",
    "    else:\n",
    "        return [], [], []\n",
    "#     if data.shape[1] != len(chans):\n",
    "#         return [], [], []\n",
    "    included.append(name)\n",
    "    f = get_freq(df)\n",
    "    if f != freq:\n",
    "        data = resample(data, df['Time'], f, freq)\n",
    "    X.append(np.array(data))\n",
    "    Y.append(0 if control else 1)\n",
    "    return X, Y, included"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(X):\n",
    "    res = []\n",
    "    for freq, time in zip(options['freqs'], options['d_units']):\n",
    "        x = resample(X, np.array([i / f for i in range(X.shape[0])]), f, freq)\n",
    "        assert(x.shape[0] >= time)\n",
    "        res.append(x[:time])\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_train_samples(samples, group, inds, units=800):\n",
    "    X = []\n",
    "    Y = []\n",
    "    \n",
    "    for j in inds:\n",
    "        sample = np.asarray(samples[j])\n",
    "        if options['normalize']:\n",
    "            sample = StandardScaler().fit_transform(sample)\n",
    "        if options['min_max']:\n",
    "#             sample = MinMaxScaler((options['min'], options['max'])).fit_transform(sample)\n",
    "            sample = sample / sample.max()\n",
    "#             sample = (sample-sample.min())/(sample.max()-sample.min())\n",
    "#             sample = (sample*(options['max']-options['min']))+options['min']\n",
    "        if options['smooth'] != 0:\n",
    "            sample = butter_lowpass_filter(sample, options['smooth'], f, options['order'])\n",
    "        offset = np.random.randint(units)\n",
    "        for i in range(units+offset, sample.shape[0], options['offset']):\n",
    "            #X.append(sample[:, i-time:i])\n",
    "            X.append(transform(sample[i-units:i, :]))\n",
    "            Y.append(1 if group[j] else 0)\n",
    "        #sample = butter_lowpass_filter(sample, 50, 200, 2)\n",
    "        #print(sample.std(axis=0), included[j])\n",
    "    res_X = []\n",
    "    for i in range(len(X[0])):\n",
    "        res_X.append(np.array([x[i] for x in X]))\n",
    "    inds = random.sample([i for i in range(len(X))], k=min(len(X), 152))\n",
    "    return res_X, np.expand_dims(Y, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SyncNetLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, options, length, i, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.options = options\n",
    "        self.Nt = length\n",
    "        self.pool_size = options['pool_size'][i]\n",
    "        self.stride = options['stride'][i]\n",
    "        self.upper_omega = options['upper_omega'][i]\n",
    "        self.lower_omega = options['lower_omega'][i]\n",
    "        self.freq = options['freqs'][i]\n",
    "        \n",
    "    def build(self, batch_input_shape):\n",
    "        self.b = self.add_weight(name=\"b\", \n",
    "                                 shape=[1,1,self.options['C'], self.options['K']], \n",
    "#                                  initializer='glorot_normal',\n",
    "                                 initializer=RandomNormal(mean=0, stddev=0.01),\n",
    "                                 regularizer = tf.keras.regularizers.l2(options['lambda_b']))\n",
    "        pi = m.pi\n",
    "        self.omega = self.add_weight(name=\"omega\", \n",
    "                                     shape=[1,1,1,self.options['K']], \n",
    "#                                      initializer='glorot_normal',\n",
    "                                     initializer=RandomUniform(minval=self.lower_omega, maxval=2*pi*self.upper_omega/self.freq),\n",
    "                                    regularizer = tf.keras.regularizers.l2(options['lambda_omega']))\n",
    "#                                      initializer='glorot_uniform')\n",
    "#                                      initializer=RandomUniform(minval=0, maxval=2*pi*self.omega/self.freq))\n",
    "        \n",
    "        self.phi = self.add_weight(name=\"phi\", \n",
    "                                  shape=[1,1,self.options['C'], self.options['K']],\n",
    "#                                   initializer='glorot_normal',\n",
    "                                   initializer=Zeros(),\n",
    "                                  regularizer = tf.keras.regularizers.l2(options['lambda_phi']))\n",
    "#                                    initializer='glorot_uniform')\n",
    "#                                    initializer=RandomNormal(mean=0, stddev=0.05))\n",
    "        \n",
    "        self.beta = self.add_weight(name=\"beta\",\n",
    "                                   shape=[1,1,1,self.options['K']],\n",
    "#                                    initializer='glorot_normal',\n",
    "                                    initializer=RandomNormal(mean=0, stddev=0.05),\n",
    "                                   regularizer = tf.keras.regularizers.l2(options['lambda_beta']),\n",
    "                                   constraint = tf.keras.constraints.NonNeg())\n",
    "#                                     initializer='glorot_uniform')\n",
    "#                                     initializer=RandomNormal(mean=0, stddev=0.05))\n",
    "        \n",
    "#         self.bias = self.add_weight(name=\"bias\",\n",
    "#                                    shape = [1])\n",
    "        \n",
    "        self.t= tf.cast(tf.reshape(range(-self.Nt//2,self.Nt//2),[1,self.Nt,1,1]), dtype=tf.float32)\n",
    "        \n",
    "        super().build(batch_input_shape)\n",
    "        \n",
    "    def call(self, X):\n",
    "        X = tf.expand_dims(X, axis=1)\n",
    "        W_osc = self.b * tf.cos(self.t*self.omega + self.phi)\n",
    "        W_decay = tf.exp(-self.t**2*self.beta)\n",
    "        W = W_osc * W_decay\n",
    "        \n",
    "        h_conv = tf.nn.relu(tf.nn.conv2d(X, W, strides=[1,1,1,1], padding='SAME'))\n",
    "#         h_conv = tf.nn.conv2d(X, W, strides=[1,1,1,1], padding='SAME')\n",
    "        self.h_pool = tf.nn.avg_pool(h_conv, ksize=[1,1,self.pool_size, 1],  strides=[1, 1, self.stride, 1], padding='SAME') #+ self.bias\n",
    "        \n",
    "        return self.h_pool\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDropout(tf.keras.layers.Layer):\n",
    "    def __init__(self, r, num, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.dropout_rate = r\n",
    "        self.num = num\n",
    "        \n",
    "    def call(self, X):\n",
    "        res = None\n",
    "        if np.random.rand() < self.dropout_rate:\n",
    "            res= tf.concat([X[:-self.num], tf.zeros_like(X[-self.num:])], axis=0)\n",
    "        else:\n",
    "            res= tf.concat([tf.zeros_like(X[:-self.num]), X[-self.num:]], axis=0)\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class TransformLayer(tf.keras.layers.Layer):\n",
    "#     def __init__(self, options, **kwargs):\n",
    "#         super().__init__(kwargs)\n",
    "#         self.options = options\n",
    "        \n",
    "#     def call(self, X):\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_eeg_ratios(df, powers, channels):    \n",
    "    cols = df.columns\n",
    "    inds = df.index\n",
    "    sudep_eeg_ratios = {}\n",
    "    c1_eeg_ratios = {}\n",
    "    c2_eeg_ratios = {}\n",
    "    rows, cols, res_cols = get_channel_names(powers, channels)\n",
    "    for subject in inds:\n",
    "        if df['asleep'][subject].shape != (1, 0) and df['awake'][subject].shape != (1, 0):\n",
    "            asleep = df['asleep'][subject][rows, cols].flatten()\n",
    "            awake = df['awake'][subject][rows, cols].flatten()\n",
    "            sudep_eeg_ratios[subject] = asleep/awake\n",
    "            \n",
    "        if df['C1_asleep'][subject].shape != (1, 0) and df['C1_awake'][subject].shape != (1, 0):\n",
    "            asleep = df['C1_asleep'][subject][rows, cols].flatten()\n",
    "            awake = df['C1_awake'][subject][rows, cols].flatten()\n",
    "            c1_eeg_ratios[subject+\"C1\"] = asleep/awake\n",
    "            \n",
    "        if df['C2_asleep'][subject].shape != (1, 0) and df['C2_awake'][subject].shape != (1, 0):\n",
    "            asleep = df['C2_asleep'][subject][rows, cols].flatten()\n",
    "            awake = df['C2_awake'][subject][rows, cols].flatten()\n",
    "            c2_eeg_ratios[subject+\"C2\"] = asleep/awake\n",
    "            \n",
    "            \n",
    "    return pd.concat([pd.DataFrame(sudep_eeg_ratios, index=res_cols).transpose(), \\\n",
    "           pd.DataFrame(c1_eeg_ratios, index=res_cols).transpose(), \\\n",
    "           pd.DataFrame(c2_eeg_ratios, index=res_cols).transpose()], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_eeg_ratios2(df, powers, channels):    \n",
    "    cols = df.columns\n",
    "    inds = df.index\n",
    "    sudep_eeg_ratios = {}\n",
    "    c1_eeg_ratios = {}\n",
    "    c2_eeg_ratios = {}\n",
    "    rows, cols, res_cols = get_channel_names(powers, channels)\n",
    "    for subject in inds:\n",
    "        if df['asleep'][subject].shape != (1, 0):\n",
    "            sudep_eeg_ratios[subject] = df['asleep'][subject][rows, cols].flatten()\n",
    "            \n",
    "        if df['C1_asleep'][subject].shape != (1, 0):\n",
    "            c1_eeg_ratios[subject+\"C1\"] = df['C1_asleep'][subject][rows, cols].flatten()\n",
    "            \n",
    "        if df['C2_asleep'][subject].shape != (1, 0):\n",
    "            c2_eeg_ratios[subject+\"C2\"] = df['C2_asleep'][subject][rows, cols].flatten()\n",
    "            \n",
    "            \n",
    "    return pd.concat([pd.DataFrame(sudep_eeg_ratios, index=res_cols).transpose(), \\\n",
    "           pd.DataFrame(c1_eeg_ratios, index=res_cols).transpose(), \\\n",
    "           pd.DataFrame(c2_eeg_ratios, index=res_cols).transpose()], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_channel_names(powers, channels):\n",
    "    res_cols = []\n",
    "    rows = []\n",
    "    cols = []\n",
    "    for power, channel in zip(powers, channels):\n",
    "        if channel == \"all\":\n",
    "            rows += [i for i in range(9)]\n",
    "            cols += [power]*9\n",
    "            res_cols += [power_to_string[power]+\" \"+str(c+1) for c in range(9)]\n",
    "        else:\n",
    "            rows += channel\n",
    "            cols += [power]*len(channel)\n",
    "            res_cols += [power_to_string[power]+\" \"+str(c+1) for c in channel]\n",
    "    return rows, cols, res_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_subject(name):\n",
    "    lower = name.lower()\n",
    "    ind = max(lower.find(\"awake\"), lower.find(\"asleep\")) - 1\n",
    "    return name[:ind]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AWAKE = \"awake\"\n",
    "ASLEEP = \"asleep\"\n",
    "\n",
    "def get_state(name):\n",
    "    if \"awake\" in name.lower():\n",
    "        return AWAKE\n",
    "    else:\n",
    "        return ASLEEP\n",
    "\n",
    "def get_ecg_ratios(df, add = \"\"):\n",
    "    inds = df.index\n",
    "    d = {}\n",
    "    ratios = {}\n",
    "    res = []\n",
    "    for i in range(inds.shape[0]):\n",
    "        name = get_subject(inds[i])\n",
    "        state = get_state(inds[i])\n",
    "        \n",
    "        if name in d:\n",
    "            if state == AWAKE:\n",
    "                ratios[name] = d[name]/df.iloc[i, :]\n",
    "            else:\n",
    "                ratios[name] = df.iloc[i, :]/d[name]\n",
    "        else:\n",
    "            d[name]=df.iloc[i, :]\n",
    "    return pd.DataFrame(ratios, index=df.columns).transpose()\n",
    "\n",
    "def get_ecg_ratios2(df, add = \"\"):\n",
    "    inds = df.index\n",
    "    d = {}\n",
    "    ratios = {}\n",
    "    res = []\n",
    "    for i in range(inds.shape[0]):\n",
    "        name = get_subject(inds[i])\n",
    "        state = get_state(inds[i])\n",
    "        \n",
    "        if state == ASLEEP:\n",
    "            ratios[name] = df.iloc[i,:]\n",
    "\n",
    "    return pd.DataFrame(ratios, index=df.columns).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine(X, Y, eeg, ecg, names):\n",
    "    res_x, res_y, res_ekg_ecg, included = [], [], [], []\n",
    "    eeg_names = set(eeg.index)\n",
    "    ecg_names = set(ecg.index)\n",
    "    assert(len(X) == len(names))\n",
    "    \n",
    "    for i in range(len(names)):\n",
    "        name = names[i]\n",
    "        name = name[:name.lower().rfind(' a')]\n",
    "        if name not in eeg_names or name not in ecg_names:\n",
    "            continue\n",
    "        res_x.append(X[i])\n",
    "        res_y.append(Y[i])\n",
    "        res_ekg_ecg.append(np.concatenate([eeg.loc[name], ecg.loc[name]]))\n",
    "        included.append(names[i])\n",
    "    return res_x, res_y, res_ekg_ecg, included"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_amps():\n",
    "    samps = []\n",
    "    camps = []\n",
    "    for i in range(len(X_train)):\n",
    "        amp = X_train[i].max()\n",
    "        if y_train[i]:\n",
    "            samps.append(amp)\n",
    "        else:\n",
    "            camps.append(amp)\n",
    "    plt.hist(samps, label=\"mean {}\".format(np.mean(samps)))\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    plt.hist(camps, label=\"mean {}\".format(np.mean(camps)))\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    samps = []\n",
    "    camps = []\n",
    "    for i in range(len(X_train)):\n",
    "        amp = X_train[i].std()\n",
    "        if y_train[i]:\n",
    "            samps.append(amp)\n",
    "        else:\n",
    "            camps.append(amp)\n",
    "    plt.hist(samps, label=\"std {}\".format(np.mean(samps)))\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    plt.hist(camps, label=\"std {}\".format(np.mean(camps)))\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eeg_dir = './EEG Files - Zhe Chen'\n",
    "paths = get_paths(eeg_dir)\n",
    "\n",
    "chan_file = 'channel_dict_more.txt'\n",
    "\n",
    "with open(chan_file,'r') as inf:\n",
    "    d = eval(inf.read())\n",
    "    \n",
    "keys = list(d.keys())\n",
    "\n",
    "if \"more\" not in chan_file:\n",
    "    for key in keys:\n",
    "        d[key[:key.rfind(' (')]+\".txt\"] = d.pop(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delta = 0\n",
    "theta = 1\n",
    "alpha = 2\n",
    "beta = 3\n",
    "low_gamma = 4\n",
    "high_gamma = 5\n",
    "\n",
    "power_to_string = {\n",
    "    delta : \"delta\",\n",
    "    theta : \"theta\",\n",
    "    alpha  :\"alpha\",\n",
    "    beta  :\"beta\",\n",
    "    low_gamma : \"low_gamma\",\n",
    "    high_gamma  :\"high_gamma\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "f =  200\n",
    "\n",
    "X_train, y_train = [], []\n",
    "X_test, y_test = [], []\n",
    "included_train = []\n",
    "included_test = []\n",
    "\n",
    "test_inst = \"melboaurne\"\n",
    "\n",
    "for path in paths:\n",
    "    if \"awake\" in path.lower():\n",
    "        continue\n",
    "    if test_inst not in path.lower():\n",
    "        X, Y, I = get_data(\"\", path, f)\n",
    "        X_train += X\n",
    "        y_train += Y\n",
    "        included_train += I\n",
    "#     else:\n",
    "#         X_test += X\n",
    "#         y_test += Y\n",
    "#         included_test += I\n",
    "    \n",
    "clear_output()\n",
    "\n",
    "print('There are {} subjects'.format(len(included_train)+len(included_test)))\n",
    "print(\"Training Set\")\n",
    "for i in included_train: print(i)\n",
    "    \n",
    "print(\"\\nTesting Set\")\n",
    "for i in included_test: print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(len(included_train)):\n",
    "#     subject = included_train[i]\n",
    "#     x = X_train[i]\n",
    "#     x[np.isnan(x)] = np.median(x[~np.isnan(x)])\n",
    "#     bound = x.mean()+4*x.std()\n",
    "#     X_train[i][x > bound] = bound\n",
    "#     X_train[i][x < -bound] = -bound\n",
    "#     sub = subject[:min(subject.find(' '), subject.find('-') if subject.find('-') > 0 else 10000000, subject.find('_') if subject.find('_') > 0 else 10000000)]\n",
    "#     if \"C1dd\" in subject:\n",
    "#         c1[sub].append(x.max())\n",
    "#     elif \"C2\" in subject:\n",
    "#         c2[sub].append(x.max())\n",
    "#     else:\n",
    "#         s[sub].append(x.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# file_dir='.\\\\ekg_features\\\\'\n",
    "\n",
    "# drop = ['nni_50', 'pnni_50', 'nni_20', 'pnni_20', 'sampen']\n",
    "\n",
    "# sudep = pd.read_excel(file_dir+\"SUDEP.xlsx\", index_col=[0], engine='openpyxl').drop(drop, axis=1)\n",
    "# c1 = pd.read_excel(file_dir+\"Control_1.xlsx\", index_col=[0], engine='openpyxl').drop(drop, axis=1)\n",
    "# c2 = pd.read_excel(file_dir+\"Control_2.xlsx\", index_col=[0], engine='openpyxl').drop(drop, axis=1)\n",
    "\n",
    "# mean_p_area = loadmat('.\\\\mat_files\\\\mean_p_area.mat')['mean_p_area']\n",
    "# for i in range(1,  mean_p_area.shape[0]):\n",
    "#     mean_p_area[i][0] = mean_p_area[i][0][0]\n",
    "# for i in range(1,  mean_p_area.shape[1]):\n",
    "#     mean_p_area[0][i] = mean_p_area[0][i][0]\n",
    "    \n",
    "# index = mean_p_area[1:, 0]\n",
    "# columns = mean_p_area[0, 1:]\n",
    "\n",
    "# mean_p_area = pd.DataFrame(mean_p_area[1:, 1:], columns=columns, index=index)\n",
    "\n",
    "# powers = []\n",
    "# channels = []\n",
    "# if True:\n",
    "#     powers = [alpha, low_gamma]\n",
    "# #     channels = [[1], [6, 8]]\n",
    "#     channels = [[1], [6]]\n",
    "# eeg = get_eeg_ratios(mean_p_area, powers, channels)\n",
    "\n",
    "# ecg_feats = []\n",
    "# if True:\n",
    "# #     ecg_feats = ['lfnu', 'hfnu', 'ratio_sd2_sd1', 'sd1']\n",
    "#     ecg_feats = ['lfnu']\n",
    "# ecg = pd.concat([get_ecg_ratios(sudep)[ecg_feats],\n",
    "#                  get_ecg_ratios(c1, \"C1\")[ecg_feats],\n",
    "#                  get_ecg_ratios(c2, \"C2\")[ecg_feats]])\n",
    "\n",
    "# X_train, y_train, ekg_eeg_train, included_train = combine(X_train, y_train, eeg, ecg, included_train)\n",
    "# X_test, y_test, ekg_eeg_test, included_test = combine(X_test, y_test, eeg, ecg, included_test)\n",
    "\n",
    "# print('There are {} subjects'.format(len(included_train)+len(included_test)))\n",
    "# print(\"Training Set\")\n",
    "# included_train.sort()\n",
    "# for i in included_train: print(i)\n",
    "    \n",
    "# print(\"\\nTesting Set\")\n",
    "# for i in included_test: print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def join(X, Y, names, i, j):\n",
    "    inds = sorted(i+j, reverse=True)\n",
    "    for a in inds:\n",
    "        X.pop(a)\n",
    "        Y.pop(a)\n",
    "        names.pop(a)\n",
    "    \n",
    "    n = len(X)\n",
    "    for _ in range(0):\n",
    "        for i in range(n):\n",
    "            x = X[i]\n",
    "            y = Y[i]\n",
    "            if y[0][0] == 1:\n",
    "                X.append(x)\n",
    "                Y.append(y)\n",
    "    \n",
    "    \n",
    "    res_X = np.concatenate(X)\n",
    "    res_Y = np.concatenate(Y)\n",
    "        \n",
    "    return res_X, res_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_roc(true, pred, points=51):\n",
    "    true = np.asarray(true)\n",
    "    pred = np.asarray(pred)\n",
    "    thresholds = np.linspace(0, 1, num=points, endpoint=True)\n",
    "    tprs = []\n",
    "    fprs = []\n",
    "    tps, tns, fps, fns = [], [], [], []\n",
    "    for i in range(thresholds.shape[0]):\n",
    "        t = thresholds[i]\n",
    "        t_pred = pred > t\n",
    "        tp = np.logical_and(t_pred==1, true==1).sum()\n",
    "        fp = np.logical_and(t_pred==1, true==0).sum()\n",
    "        tn = np.logical_and(t_pred==0, true==0).sum()\n",
    "        fn = np.logical_and(t_pred==0, true==1).sum()\n",
    "        \n",
    "        tprs.append(tp/(tp+fn))\n",
    "        fprs.append(fp/(tn+fp))\n",
    "        tps.append(tp)\n",
    "        tns.append(tn)\n",
    "        fps.append(fp)\n",
    "        fns.append(fn)\n",
    "        \n",
    "    res = {\n",
    "        'thresholds':thresholds,\n",
    "        'tprs':tprs,\n",
    "        'fprs':fprs,\n",
    "        'tps':tps,\n",
    "        'tns':tns,\n",
    "        'fps':fps,\n",
    "        'fns':fns\n",
    "    }\n",
    "    \n",
    "    for key, val in res.items():\n",
    "        res[key] = np.array(val)\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_auc(true, pred):\n",
    "    res = my_roc(true, pred)\n",
    "    return integrate(res['fprs'], res['tprs'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_loss(true, pred):\n",
    "    true =np.array(true)\n",
    "    pred = np.array(pred)\n",
    "    t = options['zero_thresh']\n",
    "    pred[pred<t] = t\n",
    "    pred[pred>(1-t)] = 1-t\n",
    "    \n",
    "    return log_loss(true, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def two_plots(loss, auc):\n",
    "    fig, ax1 = plt.subplots()\n",
    "    \n",
    "    color='tab:red'\n",
    "    ax1.set_ylabel('loss', color=color)\n",
    "    ax1.plot(loss, color=color)\n",
    "    ax1.tick_params(axis='y', labelcolor=color)\n",
    "    \n",
    "    ax2 = ax1.twinx()\n",
    "    \n",
    "    color='tab:blue'\n",
    "    ax2.set_ylabel('AUC', color=color)\n",
    "    ax2.plot(auc)\n",
    "    ax2.tick_params(axis='y', labelcolor=color)\n",
    "    ax2.axis(ymin=-0.1, ymax=1.1)\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def val_split(split=0.5, time=800):\n",
    "    train_x, train_y, val_x, val_y = [], [], [], []\n",
    "    \n",
    "    X, Y = [], []\n",
    "    \n",
    "    names = []\n",
    "    for i in pick_balanced(y_train):\n",
    "        x, y = data_X[i][0], data_Y[i]\n",
    "        X.append(x)\n",
    "        Y.append(y)\n",
    "        names.append(included_train[i])\n",
    "\n",
    "    for i in range(len(X)):\n",
    "        x, y = X[i], Y[i]\n",
    "        ind = int(len(x)*split)\n",
    "        train_x.append(x[:ind])\n",
    "        val_x.append(x[ind:])\n",
    "        train_y.append(y[:ind])\n",
    "        val_y.append(y[ind:])\n",
    "    print(train_x[0].shape)\n",
    "    print(x.shape)\n",
    "    return np.concatenate(train_x), np.concatenate(train_y), val_x, val_y, names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_val_test_inds(y):\n",
    "    inds = [i for i in range(len(y))]\n",
    "    if options['balanced']:\n",
    "        inds = [inds[i] for i in pick_balanced(y)]\n",
    "        y = [y[i] for i in inds]\n",
    "    train_x, val_x, train_y, val_y = train_test_split(inds, y, \n",
    "                                                      test_size = options['val_split'], stratify=y)\n",
    "    \n",
    "    \n",
    "    \n",
    "    train_x, test_x, train_y, test_y = train_test_split(train_x, train_y, \n",
    "                                                        test_size=options['test_split']/(1-options['val_split']),\n",
    "                                                        stratify = train_y)\n",
    "    test_x = [test_x[i] for i in pick_balanced(test_y)]\n",
    "    \n",
    "    return train_x, val_x, test_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def helper(train_x, train_y, val_x, val_y, test_x, test_y, train_features, val_features, test_features):\n",
    "    X_train_np = np.vstack(train_x)\n",
    "    y_train_np = np.vstack(train_y)\n",
    "    e_train_np = np.vstack(train_features)\n",
    "    \n",
    "    min_loss = float('inf')\n",
    "    \n",
    "    model = create_model(combo[1], 9, train_features[0].shape[1])\n",
    "    \n",
    "    p = 0\n",
    "    \n",
    "    true, pred = [], []\n",
    "    \n",
    "    for i in range(1, options['epochs']+1):\n",
    "        inds = np.random.choice(X_train_np.shape[0], size=batch_size)\n",
    "        model.fit([X_train_np, e_train_np], y_train_np, verbose=1)\n",
    "        if i % options['valid'] == 0:\n",
    "            curr_val_loss, _, _, _ = test(val_x, val_y, val_features, model)\n",
    "            print(\"val_loss\", curr_val_loss)\n",
    "            if curr_val_loss < min_loss:\n",
    "                min_loss = curr_val_loss\n",
    "                _, _, true, pred = test(test_x, test_y, test_features, model)\n",
    "                p = 0\n",
    "            elif p >= options['patience']:\n",
    "                break\n",
    "            else:\n",
    "                p += 1\n",
    "                \n",
    "    return true, pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_roc(data, test, folder):\n",
    "    save_name=folder\n",
    "\n",
    "    fontsize={\n",
    "        'title':50,\n",
    "        'axis':50,\n",
    "        'ticks':40,\n",
    "        'legend':40\n",
    "    }\n",
    "    \n",
    "    copy = {}\n",
    "\n",
    "    for key, val in data.items():\n",
    "        copy[key] = [val[i].tolist() for i in range(len(val))]\n",
    "        \n",
    "    with open(f'.\\ROC\\\\{save_name}\\\\{test} results {combo[0]}-{combo[1]}.txt', 'w') as f:\n",
    "        print(copy, file=f)\n",
    "\n",
    "    tprs = np.array(data['tprs'])\n",
    "    for key, val in copy.items():\n",
    "        copy[key] = np.array(val).mean(axis=0)\n",
    "\n",
    "    mean_fpr = copy['fprs']\n",
    "    mean_tpr = copy['tprs']\n",
    "\n",
    "    plt.figure(figsize=(15,12))\n",
    "\n",
    "    plt.tick_params(labelsize=fontsize['ticks'])\n",
    "    plt.title(\"ROC Syncnet\", fontsize=fontsize['title'])\n",
    "    plt.xlabel('False Positive Rate', fontsize=fontsize['axis'])\n",
    "    plt.ylabel('True Positive Rate', fontsize=fontsize['axis'])\n",
    "\n",
    "    area = integrate(mean_fpr, mean_tpr)\n",
    "    plt.plot(mean_fpr, mean_tpr, label=\"Area: {:.3f}\".format(area), linewidth=3)\n",
    "    plt.plot([0, 1], [0, 1], linestyle='dashed')\n",
    "\n",
    "    CIs = CI(tprs)\n",
    "    #plt.vlines(x=mean_fpr, ymin=CIs[:, 0], ymax=CIs[:, 1], color=\"black\", linewidth=3)\n",
    "    #plt.hlines(y=CIs[:, 0], xmin=mean_fpr-width, xmax=mean_fpr+width, color=\"black\", linewidth=3)\n",
    "    #plt.hlines(y=CIs[:, 1], xmin=mean_fpr-width, xmax=mean_fpr+width, color=\"black\", linewidth=3)\n",
    "    plt.fill_between(mean_fpr, CIs[:, 0], CIs[:, 1], alpha=0.2)\n",
    "    plt.legend(loc='upper left', fontsize=fontsize['legend'])\n",
    "    data[\"lower\"] = CIs[:, 0]\n",
    "    data[\"upper\"] = CIs[:, 1]\n",
    "    pd.DataFrame(copy).to_excel(f\".\\ROC\\\\{save_name}\\\\Confidence Interval, {combo[0]}-{combo[1]}, {test}.xlsx\", index=False)\n",
    "\n",
    "    plt.savefig(f\".\\ROC\\\\{save_name}\\\\ROC Curve {combo[0]}-{combo[1]}, {test}.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as st\n",
    "\n",
    "def mean_confidence_interval(data, confidence=0.95):\n",
    "    a = 1.0 * np.array(data)\n",
    "    n = len(a)\n",
    "    m, se = np.mean(a), scipy.stats.sem(a)\n",
    "    h = se * scipy.stats.t.ppf((1 + confidence) / 2., n-1)\n",
    "    return m, m-h, m+h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(X, Y, F, model):\n",
    "    true, pred = [], []\n",
    "    for x, y, f in zip(X, Y, F):\n",
    "        p = model.predict(x+[f])\n",
    "        pred.append(np.mean(p))\n",
    "        true.append(y[0][0])\n",
    "    return my_loss(true, pred), my_auc(true, pred), true, pred\n",
    "\n",
    "def actual_test(X, Y, F, model):\n",
    "    true = np.concatenate(Y)\n",
    "    pred = model.predict([np.concatenate([x[i] for x in X]) for i in range(len(X[0]))]+[np.vstack(F)])\n",
    "    return my_loss(true, pred), my_auc(true, pred), true, pred\n",
    "\n",
    "def MC_test(X, Y, F, model):\n",
    "    with tf.keras.backend.learning_phase_scope(1):\n",
    "        pred = np.stack([[model.predict([x, f]).mean() for x, f in zip(X, F)] for _ in range(options['MC_num_iters'])])\n",
    "    return [y[0][0] for y in Y], list(pred.mean(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_run(ekg_eeg_feats):\n",
    "    train, val, testi = train_val_test_inds(y_train)\n",
    "    train_x, train_y, val_x, val_y, test_x, test_y = [], [], [], [], [], []\n",
    "    train_features, val_features, test_features = [], [], []\n",
    "    \n",
    "    print(\"training:\")\n",
    "    for i in train:\n",
    "#         x, y = to_train_samples(X_train, y_train, [i], options['units'])\n",
    "        x, y = data_X[i], data_Y[i]\n",
    "        train_x.append(x)\n",
    "        train_y.append(y)\n",
    "        train_features.append(np.vstack([ekg_eeg_feats[i] for _ in range(x[0].shape[0])]))\n",
    "        \n",
    "        print(included_train[i][:-4])\n",
    "    print()\n",
    "    print(\"validation:\")\n",
    "    for i in val:\n",
    "#         x, y = to_train_samples(X_train, y_train, [i], options['units'])\n",
    "        x, y = data_X[i], data_Y[i]\n",
    "        val_x.append(x)\n",
    "        val_y.append(y)\n",
    "        val_features.append(np.vstack([ekg_eeg_feats[i] for _ in range(x[0].shape[0])]))\n",
    "        \n",
    "        print(included_train[i][:-4])\n",
    "    print()\n",
    "        \n",
    "    test_names = []\n",
    "    print(\"testing:\")\n",
    "    for i in testi:\n",
    "#         x, y = to_train_samples(X_train, y_train, [i], options['units'])\n",
    "        x, y = data_X[i], data_Y[i]\n",
    "        test_x.append(x)\n",
    "        test_y.append(y)\n",
    "        test_features.append(np.vstack([ekg_eeg_feats[i] for _ in range(x[0].shape[0])]))\n",
    "        test_names.append(included_train[i])\n",
    "        \n",
    "        print(included_train[i][:-4])\n",
    "    print()\n",
    "        \n",
    "    num_repeated = 0\n",
    "    \n",
    "    all_info = {\n",
    "        \"names\": [],\n",
    "        \"prediction\": [],\n",
    "        \"actual\": []\n",
    "    }\n",
    "    model = None \n",
    "    while True:\n",
    "        try:\n",
    "            tf.keras.backend.clear_session()\n",
    "            callback = tf.keras.callbacks.EarlyStopping(\n",
    "                    monitor=\"val_loss\",\n",
    "                    min_delta=0,\n",
    "                    patience=options['patience'],\n",
    "                    verbose=0,\n",
    "                    mode=\"auto\",\n",
    "                    baseline=None,\n",
    "                    restore_best_weights=True,\n",
    "                )\n",
    "            model = create_model(options['C'], 0)\n",
    "#                 X_train_np = np.concatenate(train_x)\n",
    "#                 X_val_np = np.concatenate(val_x)\n",
    "            model.fit([np.concatenate([x[i] for x in train_x]) for i in range(len(train_x[0]))]+[np.vstack(train_features)], \n",
    "                  np.concatenate(train_y), \n",
    "                  epochs=options['epochs'],\n",
    "                  batch_size=options['batch_size'],\n",
    "                  validation_data=([np.concatenate([x[i] for x in val_x]) for i in range(len(val_x[0]))]+[np.vstack(val_features)], np.concatenate(val_y)),\n",
    "                  verbose=options['verbose'],\n",
    "                  callbacks=[callback],\n",
    "                  class_weight=options['class_weight'])\n",
    "            val_auc = actual_test(val_x, val_y, val_features, model)[1]\n",
    "            if val_auc == 0.5 and num_repeated < 10:\n",
    "                num_repeated += 1\n",
    "                print(\"repeat\", num_repeated)\n",
    "                continue\n",
    "            break\n",
    "        except KeyboardInterrupt:\n",
    "            return None, None\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            del model\n",
    "            continue\n",
    "\n",
    "\n",
    "\n",
    "    _,_,t,p = test(test_x, test_y, test_features, model)\n",
    "#         t, p = ensemble(train_x, train_y, val_x, val_y, test_x, test_y, train_features, val_features, test_features, test_names)\n",
    "    test_auc = roc_auc_score(t, p)\n",
    "\n",
    "    all_info['names'].append(test_names)\n",
    "    all_info['prediction'].append(p)\n",
    "    all_info['actual'].append(t)\n",
    "    \n",
    "    return None, test_auc, all_info, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def n_fold_one_run(train_x, train_y, val_x, val_y, test_x, test_y, train_features, val_features, test_features):\n",
    "    num_repeated = 0\n",
    "        \n",
    "    while True:\n",
    "        try:\n",
    "            tf.keras.backend.clear_session()\n",
    "            callback = tf.keras.callbacks.EarlyStopping(\n",
    "                    monitor=\"val_loss\",\n",
    "                    min_delta=0,\n",
    "                    patience=options['patience'],\n",
    "                    verbose=0,\n",
    "                    mode=\"auto\",\n",
    "                    baseline=None,\n",
    "                    restore_best_weights=True,\n",
    "                )\n",
    "            model = create_model(options['C'], train_features[0].shape[1])\n",
    "#                 X_train_np = np.concatenate(train_x)\n",
    "#                 X_val_np = np.concatenate(val_x)\n",
    "            model.fit([np.concatenate([x[i] for x in train_x]) for i in range(len(train_x[0]))]+[np.vstack(train_features)], \n",
    "                  np.concatenate(train_y), \n",
    "                  epochs=options['epochs'],\n",
    "                  batch_size=options['batch_size'],\n",
    "                  validation_data=([np.concatenate([x[i] for x in val_x]) for i in range(len(val_x[0]))]+[np.vstack(val_features)], np.concatenate(val_y)),\n",
    "                  verbose=options['verbose'],\n",
    "                  callbacks=[callback])\n",
    "            val_auc = actual_test(val_x, val_y, val_features, model)[1]\n",
    "            if val_auc == 0.5 and num_repeated < 10:\n",
    "                num_repeated += 1\n",
    "                print(\"repeat\", num_repeated)\n",
    "                continue\n",
    "            break\n",
    "        except KeyboardInterrupt:\n",
    "            return None, None\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            del model\n",
    "            continue\n",
    "    _,_,t,p = test(test_x, test_y, test_features, model)\n",
    "    print(model.layers[-1].get_weights())\n",
    "    del model\n",
    "    return t,p\n",
    "\n",
    "def split(X, Y, names, data_features, train_index, test_index):\n",
    "    train_x = [X[i] for i in train_index]\n",
    "    train_y = [Y[i] for i in train_index]\n",
    "    data_features = np.array(data_features)\n",
    "#     if data_features.shape[1] != 0:\n",
    "#         scaler = MinMaxScaler(feature_range=(options['min'],options['max']))\n",
    "#         data_features[train_index] = scaler.fit_transform(data_features[train_index])\n",
    "#         data_features[test_index] = scaler.transform(data_features[test_index])\n",
    "    train_features = [np.vstack([data_features[i] for _ in range(X[i][0].shape[0])]) for i in train_index]\n",
    "    \n",
    "    test_x = [X[i] for i in test_index]\n",
    "    test_y = [Y[i] for i in test_index]\n",
    "    test_features = [np.vstack([data_features[i] for _ in range(X[i][0].shape[0])]) for i in test_index]\n",
    "    test_names = [names[i] for i in test_index]\n",
    "    \n",
    "    train_x, val_x, train_y, val_y, train_features, val_features = train_test_split(train_x, train_y, train_features, test_size = options['val_split'], stratify=[y[0][0] for y in train_y])\n",
    "    \n",
    "    return train_x, train_y, val_x, val_y, test_x, test_y, train_features, val_features, test_features, test_names\n",
    "\n",
    "def nfold(ekg_eeg_feats, folds=5, fold_num=0):\n",
    "    \n",
    "    def my_test(model):\n",
    "        pred = []\n",
    "        true = []\n",
    "        \n",
    "        for x, y, f in zip(test_x, test_y, test_features):\n",
    "            p = model.predict([x[:,i,:,:] for i in range(x.shape[1])]+[f]).mean()\n",
    "            pred.append(p)\n",
    "        return [y[0][0] for y in test_y], pred\n",
    "    \n",
    "    skf = StratifiedKFold(shuffle=True, n_splits=folds)\n",
    "    \n",
    "    all_info = {\n",
    "        \"names\": [],\n",
    "        \"prediction\": [],\n",
    "        \"actual\": []\n",
    "    }\n",
    "    \n",
    "    X, Y, data_features, names = [], [], [], []\n",
    "    asd = []\n",
    "    min_samples = 20000\n",
    "    for i in pick_balanced(y_train):\n",
    "        x, y = data_X[i], data_Y[i]\n",
    "#         x, y = to_train_samples(X_train, y_train, [i], options['units'])\n",
    "        min_samples = min(x[0].shape[0], min_samples)\n",
    "        X.append(x)\n",
    "        Y.append(y)\n",
    "        data_features.append(ekg_eeg_feats[i])\n",
    "        names.append(included_train[i])\n",
    "        asd.append(y[0][0])\n",
    "    \n",
    "#     for i in range(len(X)):\n",
    "#         inds = np.random.choice(X[i][0].shape[0], min_samples, replace=False)\n",
    "#         X[i] = [x[inds] for x in X[i]]\n",
    "#         Y[i] = Y[i][inds]\n",
    "    \n",
    "    dummy = [i for i in range(len(asd))]\n",
    "    i = 1\n",
    "    true, pred = [], []\n",
    "    \n",
    "    auc_sum = 0\n",
    "    \n",
    "#     for train_index, test_index in five_fold_ind(predefined_folds[fold_num]):\n",
    "    for train_index, test_index in skf.split(dummy, asd):\n",
    "        print(f\"fold {i}/{folds}\")\n",
    "        i+=1\n",
    "        train_x, train_y, val_x, val_y, test_x, test_y, train_features, val_features, test_features, test_names = split(X, Y, names, data_features, train_index, test_index)        \n",
    "        t,p = n_fold_one_run(train_x, train_y, val_x, val_y, test_x, test_y, train_features, val_features, test_features)\n",
    "        test_auc = roc_auc_score(t, p)\n",
    "        print(t)\n",
    "        print(p)\n",
    "        auc_sum += test_auc\n",
    "        print(test_auc, auc_sum/(i-1))\n",
    "        \n",
    "        true += t\n",
    "        pred += p\n",
    "\n",
    "        all_info['names'].append(test_names)\n",
    "        all_info['prediction'].append(p)\n",
    "        all_info['actual'].append(t)\n",
    "    return my_roc(true, pred), auc_sum / folds, all_info\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(channels, num_features):\n",
    "    features_input = tf.keras.layers.Input(shape=(num_features,), name=\"features_input\")\n",
    "    all_inputs = []\n",
    "    all_logits = []\n",
    "    \n",
    "    for i in range(len(options['freqs'])):    \n",
    "        raw_eeg_input = tf.keras.layers.Input(shape=(options['d_units'][i], channels), name=\"raw_eeg_input_{}\".format(i+1))\n",
    "        all_inputs.append(raw_eeg_input)\n",
    "        raw_eeg_input = tf.keras.layers.SpatialDropout1D(options['spatial_dropout'])(raw_eeg_input)\n",
    "        syncnet_layer = SyncNetLayer(options, options['filter_sizes'][i], i)(raw_eeg_input)\n",
    "        logits = tf.keras.layers.Flatten()(syncnet_layer)\n",
    "#         logits = tf.keras.layers.ReLU(max_value=None)(logits)\n",
    "#         logits  = tf.keras.layers.Lambda(lambda x: 5*(1/(1+tf.exp(-0.1*x))))(logits)\n",
    "#         logits = tf.keras.layers.BatchNormalization()(logits)\n",
    "        all_logits.append(logits)\n",
    "    \n",
    "    if num_features!=0:\n",
    "        all_inputs.append(features_input)\n",
    "        all_logits.append(features_input)\n",
    "#     print(all_logits)\n",
    "    if len(all_logits) > 1:\n",
    "        concat = tf.keras.layers.Concatenate(axis=1)(all_logits)\n",
    "    else:\n",
    "        concat = all_logits[0]\n",
    "#     concat = tf.keras.layers.BatchNormalization()(concat)\n",
    "    dropout = tf.keras.layers.Dropout(options['dropout'])(concat)\n",
    "    \n",
    "#     dropout = tf.keras.layers.Dense(32, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(options['lambda']))(dropout)\n",
    "#     dropout = tf.keras.layers.Dropout(options['dropout'])(dropout)\n",
    "    \n",
    "#     dropout = tf.keras.layers.Dense(4, activation='selu',kernel_regularizer=tf.keras.regularizers.l2(options['lambda']))(dropout)\n",
    "#     dropout = tf.keras.layers.Dropout(options['dropout'])(dropout)\n",
    "    \n",
    "    out = tf.keras.layers.Dense(1, activation='sigmoid',\n",
    "                                kernel_regularizer=tf.keras.regularizers.l2(options['lambda']))(dropout)\n",
    "    model = tf.keras.Model(inputs=all_inputs, outputs=out, name=\"SyncNet\")\n",
    "    \n",
    "#     options['lr'] = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "#                                         initial_learning_rate=0.5,\n",
    "#                                         decay_steps=10000,\n",
    "#                                         decay_rate=0.99)\n",
    "\n",
    "\n",
    "    options['optimizer'] = tf.keras.optimizers.Adam(options['lr'])\n",
    "    \n",
    "    model.compile(loss=\"binary_crossentropy\", optimizer=options['optimizer'], metrics=['AUC'])\n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "options = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "window = 400\n",
    "\n",
    "options['offset'] = window\n",
    "options['select'] = False\n",
    "options['units'] = window\n",
    "options['d_units'] = [window]\n",
    "options['normalize'] = False\n",
    "options['min_max'] = False\n",
    "options['max'] = 1\n",
    "options['min'] = 0\n",
    "options['smooth'] = 0\n",
    "options['order'] = 0\n",
    "options['freqs'] = [200]\n",
    "\n",
    "data_X, data_Y = [], []\n",
    "\n",
    "for i in range(len(X_train)):\n",
    "    print(f\"{i+1}/{len(X_train)}\")\n",
    "    x, y = to_train_samples(X_train, y_train, [i], options['units'])\n",
    "    data_X.append(x)\n",
    "    data_Y.append(y)\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ekg_eeg_train = [[] for _ in range(len(X_train))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_test():\n",
    "    \n",
    "    def test(X, Y):\n",
    "        true, pred = [], []\n",
    "        for x, y in zip(X, Y):\n",
    "#             print(x.shape)\n",
    "            p = model.predict(x)\n",
    "            pred.append(p.mean())\n",
    "            true.append(y[0][0])\n",
    "        return my_loss(true, pred), my_auc(true, pred), true, pred\n",
    "    \n",
    "    def actual_test(X, Y):\n",
    "        true = np.concatenate(Y)\n",
    "        pred = model.predict(np.concatenate(X))\n",
    "        return my_loss(true, pred), my_auc(true, pred), true, pred\n",
    "    \n",
    "    \n",
    "    x_train, y_train, x_val, y_val, names = val_split()\n",
    "    model = create_model(options['C'], 0)\n",
    "    \n",
    "    callback = tf.keras.callbacks.EarlyStopping(\n",
    "                monitor=\"val_loss\",\n",
    "                min_delta=0,\n",
    "                patience=10,\n",
    "                verbose=0,\n",
    "                mode=\"auto\",\n",
    "                baseline=None,\n",
    "                restore_best_weights=True,\n",
    "               )\n",
    "\n",
    "    model.fit(x_train, \n",
    "              y_train, \n",
    "              batch_size=options['batch_size'], \n",
    "              epochs=options['epochs'], \n",
    "              callbacks=[callback],\n",
    "             verbose=options['verbose'],\n",
    "             validation_split=0.1)\n",
    "    \n",
    "    _,_,true, pred = test(x_val, y_val)\n",
    "    \n",
    "    info = {\n",
    "        \"names\":names,\n",
    "        \"prediction\":pred,\n",
    "        \"actual\":true\n",
    "    }\n",
    "    \n",
    "    return None, roc_auc_score(true, pred), info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "included_feats = [[] for i in range(len(included_train))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#samples\n",
    "options['epochs'] = 1000\n",
    "options['num_labels'] = 1\n",
    "options['batch_size'] = 128\n",
    "#conv2d\n",
    "options['C'] = X_train[0].shape[1]\n",
    "options['K'] = 4\n",
    "options['filter_sizes'] = [window//2]\n",
    "options['pool_size'] = [window]\n",
    "options['stride'] = [window]\n",
    "\n",
    "#regularization\n",
    "options['dropout'] = 0.5\n",
    "options['spatial_dropout'] = 0.\n",
    "\n",
    "options['lambda'] = 0.0\n",
    "options['lambda_2'] = 0.0\n",
    "options['lambda_b'] = 0\n",
    "options['lambda_omega'] =0\n",
    "options['lambda_beta'] = 0\n",
    "options['lambda_phi'] =0\n",
    "options['lr'] = 0.1\n",
    "\n",
    "options['stddev'] = 0.0\n",
    "options['MC_num_iters'] = 20\n",
    "options['early_stop'] = True\n",
    "options['patience'] = 10\n",
    "options['valid'] = 1\n",
    "\n",
    "#idk\n",
    "options['upper_omega']= [50, 50]\n",
    "options['lower_omega'] = [0, 0]\n",
    "options['class_weight'] = {0:1.0, 1:1.0}\n",
    "# options['lr'] = 0.005\n",
    "\n",
    "options['freq'] = f\n",
    "\n",
    "#data sets\n",
    "options['test_split'] = .2\n",
    "options['val_split'] = 0.1\n",
    "\n",
    "options['zero_thresh'] = 1e-5\n",
    "\n",
    "options['select'] = False\n",
    "options['beta1'] = 0.9 #default is 0.9\n",
    "options['beta2'] = 0.999 #default is 0.999\n",
    "options['balanced'] = False\n",
    "options['verbose'] = 1\n",
    "\n",
    "\n",
    "\n",
    "num_trials = 100\n",
    "# for feats, save_name in zip([(0,0),(0,3)], ['Raw Data', 'Raw Data + EEG Feats']):\n",
    "for feats, save_name in zip([(0,0)], ['Raw Data']):\n",
    "# for feats, save_name in zip([(0,3)], ['Raw Data + EEG Feats']):\n",
    "# for feats, save_name in zip([(0,3)], ['Raw Data + EEG + EKG Feats']):\n",
    "    all_infos_arr = []\n",
    "    auc = []\n",
    "    \n",
    "    included_feats = [a[feats[0]:feats[1]] for a in ekg_eeg_train]\n",
    "    i = len(all_infos_arr)\n",
    "    while i < num_trials:\n",
    "        print(i+1)\n",
    "        try:\n",
    "#             res, res_auc, info = one_run_two(included_feats)\n",
    "#             res, res_auc, info = one_run(included_feats)\n",
    "            res, res_auc, info = nfold(included_feats, 5, i)\n",
    "#             res, res_auc, info = split_test()\n",
    "        except KeyboardInterrupt:\n",
    "            break\n",
    "#         except Exception as e:\n",
    "#             print(e)\n",
    "#             continue\n",
    "\n",
    "        clear_output()\n",
    "#         auc.append(integrate(res['fprs'], res['tprs']))\n",
    "        auc.append(res_auc)\n",
    "        print(mean_confidence_interval(auc))\n",
    "        print(np.median(auc))\n",
    "        print(res_auc)\n",
    "        plt.hist(auc)\n",
    "        plt.show()\n",
    "        all_infos_arr.append(info)\n",
    "        i += 1\n",
    "#         with open('.\\ROC\\\\5-fold Cross Validation Final\\\\{} Results 8s other.txt'.format(save_name), 'w') as g:\n",
    "#             print(all_infos_arr, file=g)\n",
    "#     if len(all_infos_arr) == num_trials:\n",
    "#         with open('.\\ROC\\\\5-fold Cross Validation Final\\\\{} Results 8s other.txt'.format(save_name), 'w') as g:\n",
    "#             print(all_infos_arr, file=g)\n",
    "#     save_roc(data, save_name)\n",
    "    plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
